{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IRE project phase 2 simple sadness.ipynb","provenance":[{"file_id":"1SmwarEuWDjkhtP8gc1hxcqP0wGFZ9Cen","timestamp":1637215397123},{"file_id":"1-hsWdDR29qCtG0w6g87heImXa0RGUoqh","timestamp":1636958710803}],"collapsed_sections":["J_zxz1zyD4PM","MvAwUXx-ZGZA"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"21136d4c2ad941b3b1dcbaf402a0974b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_69f7dc2c70864723809fca642fc68c0e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c10707a6ba844bd4a2413bbb199dd8e3","IPY_MODEL_27f8cc1a61374997868177b5996be2ba","IPY_MODEL_f72b5394766d432a977658114acbac01"]}},"69f7dc2c70864723809fca642fc68c0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c10707a6ba844bd4a2413bbb199dd8e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2bc8d206249a43c59f8ccf1040713ec8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c79844fee0ed40249611dbd3e3276509"}},"27f8cc1a61374997868177b5996be2ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c1204727b6b944f39cfbb388d681c252","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":501200538,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":501200538,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2b6c89c1b37a429282a76277288b379c"}},"f72b5394766d432a977658114acbac01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_76b14fd52ec94af593c9af71413ce1dd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 478M/478M [00:16&lt;00:00, 30.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ce50ec05d23d42b885cc709361a6f6ad"}},"2bc8d206249a43c59f8ccf1040713ec8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c79844fee0ed40249611dbd3e3276509":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c1204727b6b944f39cfbb388d681c252":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2b6c89c1b37a429282a76277288b379c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"76b14fd52ec94af593c9af71413ce1dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ce50ec05d23d42b885cc709361a6f6ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"1KF70UjGDtiU"},"source":["# IRE Project phase 2 (sadness)\n","## Fine tune transformer for emotion intensity regression"]},{"cell_type":"markdown","metadata":{"id":"J_zxz1zyD4PM"},"source":["## Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oBB8eyGXDkGs","executionInfo":{"status":"ok","timestamp":1637216482843,"user_tz":-330,"elapsed":3805,"user":{"displayName":"Aryamaan Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKvxqXMjfEQPe4K-2M9GvOVADF5IAW14NQ8-c2Yw=s64","userId":"18320413729977503417"}},"outputId":"a15575cc-6035-4ec3-c55c-2d110e173c1f"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"id":"_2Dobkad3W54"},"source":["# Importing the libraries needed\n","import pandas as pd\n","import numpy as np\n","import torch\n","import transformers\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import DistilBertModel, DistilBertTokenizer\n","import re\n","from scipy.stats import pearsonr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RUAapitZDRkG","executionInfo":{"status":"ok","timestamp":1637216486483,"user_tz":-330,"elapsed":15,"user":{"displayName":"Aryamaan Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKvxqXMjfEQPe4K-2M9GvOVADF5IAW14NQ8-c2Yw=s64","userId":"18320413729977503417"}},"outputId":"dceb8023-a63b-49bc-de39-f59a0ab79661"},"source":["# Setting up the device for GPU usage\n","\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"XdVz5ib2EBLr"},"source":["## Data handling"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7BxuKcqQDd8j","executionInfo":{"status":"ok","timestamp":1637216487496,"user_tz":-330,"elapsed":1024,"user":{"displayName":"Aryamaan Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKvxqXMjfEQPe4K-2M9GvOVADF5IAW14NQ8-c2Yw=s64","userId":"18320413729977503417"}},"outputId":"cdbc077e-2a12-487e-f726-46e892b01d2c"},"source":["!wget http://www.saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/EI-reg/English/EI-reg-En-train.zip\n","!wget http://saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/EI-reg/English/2018-EI-reg-En-dev.zip\n","!unzip -q /content/EI-reg-En-train.zip\n","!unzip -q /content/2018-EI-reg-En-dev.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-11-18 06:21:18--  http://www.saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/EI-reg/English/EI-reg-En-train.zip\n","Resolving www.saifmohammad.com (www.saifmohammad.com)... 192.185.17.122\n","Connecting to www.saifmohammad.com (www.saifmohammad.com)|192.185.17.122|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 356461 (348K) [application/zip]\n","Saving to: ‘EI-reg-En-train.zip’\n","\n","EI-reg-En-train.zip 100%[===================>] 348.11K  --.-KB/s    in 0.1s    \n","\n","2021-11-18 06:21:18 (2.79 MB/s) - ‘EI-reg-En-train.zip’ saved [356461/356461]\n","\n","--2021-11-18 06:21:18--  http://saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/EI-reg/English/2018-EI-reg-En-dev.zip\n","Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n","Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 83779 (82K) [application/zip]\n","Saving to: ‘2018-EI-reg-En-dev.zip’\n","\n","2018-EI-reg-En-dev. 100%[===================>]  81.82K  --.-KB/s    in 0.06s   \n","\n","2021-11-18 06:21:19 (1.26 MB/s) - ‘2018-EI-reg-En-dev.zip’ saved [83779/83779]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"iCnf84PiFP1Y"},"source":["# Defining some key variables that will be used later on in the training\n","MAX_LEN = 64\n","TRAIN_BATCH_SIZE = 2\n","VALID_BATCH_SIZE = 1\n","EPOCHS = 10\n","LEARNING_RATE = 1e-05\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained('roberta-base')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ig8g1xxrFS3Y"},"source":["def pre_process(tweet):\n","  tweet = tweet.lower()\n","  tweet = tweet.encode(\"ascii\", \"ignore\").decode() # removes emoticons and non-English characters\n","  tweet = re.sub(r\"@{1}[a-z0-9_]+\\s\", \" \", tweet) # removes username mentions\n","  tweet = re.sub(r\"htt(p|ps)\\S+\", \" \", tweet) # removes links in the tweet\n","  tweet = re.sub(r'[a-z0-9._%-]+@[a-z0-9.-]+\\.[a-z]{2,4}', \" \", tweet) # removes email\n","  tweet = re.sub(r\"#\", \"\", tweet)\n","  tweet = re.sub(r\"\\\\n|\\\\t\", \" \", tweet)\n","  tweet = \" \".join(tweet.split()) # removing multiple spaces between words\n","  return tweet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4RBVeHJkED2Q"},"source":["def load_dataset(emotion):\n","  train_data = pd.read_csv(f\"EI-reg-En-{emotion}-train.txt\", delimiter='\\t')\n","  test_data = pd.read_csv(f\"2018-EI-reg-En-{emotion}-dev.txt\", delimiter='\\t')\n","  \n","  train_sentences = train_data[\"Tweet\"].tolist()\n","  train_sentences = list(map(pre_process, train_sentences))\n","  y_train = train_data[\"Intensity Score\"].to_numpy().reshape((-1, 1))\n","  \n","  test_sentences = test_data[\"Tweet\"].tolist()\n","  test_sentences = list(map(pre_process, test_sentences))\n","  y_test = test_data[\"Intensity Score\"].to_numpy().reshape((-1, 1))\n","\n","  return train_sentences, y_train, test_sentences, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H8i_gC3DFlo2"},"source":["class Triage(Dataset):\n","    def __init__(self, X_train, y_train, tokenizer, max_len):\n","        self.len = len(X_train)\n","        self.X_train = X_train\n","        self.y_train = y_train\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        \n","    def __getitem__(self, index):\n","        inputs = self.tokenizer.encode_plus(\n","            self.X_train[index],\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_token_type_ids=True,\n","            truncation=True\n","        )\n","\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'targets': torch.tensor(self.y_train[index], dtype=torch.float)\n","        } \n","    \n","    def __len__(self):\n","        return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3cDOpq96Fqiv"},"source":["train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","test_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': False,\n","                'num_workers': 0\n","                }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MvAwUXx-ZGZA"},"source":["## Network"]},{"cell_type":"code","metadata":{"id":"UEBiNow6Y4zh"},"source":["# Creating the customized model, by adding a drop out and a dense layer on top of Roberta to get the final output for the model. \n","from transformers import AutoModel\n","class RobertaBERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(RobertaBERTClass, self).__init__()\n","        self.l1 = AutoModel.from_pretrained(\"roberta-base\")\n","        # self.fc = torch.nn.Sequential(\n","        #     torch.nn.Linear(768,768),\n","        #     torch.nn.ReLU(),\n","        #     torch.nn.Linear(768,1),\n","        #     # torch.nn.ReLU(),\n","        #     # torch.nn.Linear(128,32),            \n","        #     # torch.nn.ReLU(),\n","        #     # torch.nn.Linear(32,1)\n","        # )\n","        self.pre_classifier = torch.nn.Linear(768, 768)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.classifier = torch.nn.Linear(768, 1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_state = output_1[0]\n","        pooler = hidden_state[:, 0]\n","        # output = self.fc(pooler)\n","        pooler = self.pre_classifier(pooler)\n","        pooler = torch.nn.ReLU()(pooler)\n","        pooler = self.dropout(pooler)\n","        output = self.classifier(pooler)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-7iD1BZOZpAT"},"source":["loss_function = torch.nn.MSELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDpjSgTcZwJ-"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"dbkau-_MZtKK"},"source":["# Defining the training function on the 80% of the dataset for tuning the Roberta model\n","\n","def train(epoch):\n","    tr_loss = 0\n","    nb_tr_steps = 0\n","    nb_tr_examples = 0\n","    model.train()\n","    for _,data in enumerate(training_loader, 0):\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device)\n","\n","        outputs = model(ids, mask)\n","        loss = loss_function(outputs, targets)\n","        tr_loss += loss.item()\n","        nb_tr_steps += 1\n","        nb_tr_examples+=targets.size(0)\n","        \n","        # if _%50==0:\n","        #     loss_step = tr_loss/nb_tr_steps\n","        #     print(f\"Training Loss per 50 steps: {loss_step}\")\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss/nb_tr_steps\n","    print(f\"Training Loss Epoch: {epoch_loss}\")\n","\n","    y_true = []\n","    y_pred = []\n","\n","    model.eval()\n","    for _,data in enumerate(testing_loader, 0):\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(ids, mask)\n","\n","        y_true.append(data['targets'].cpu().item())\n","        y_pred.append(outputs.cpu().item())\n","\n","    print('pearsonr score:', pearsonr(y_true, y_pred)[0])\n","\n","    return "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321,"referenced_widgets":["21136d4c2ad941b3b1dcbaf402a0974b","69f7dc2c70864723809fca642fc68c0e","c10707a6ba844bd4a2413bbb199dd8e3","27f8cc1a61374997868177b5996be2ba","f72b5394766d432a977658114acbac01","2bc8d206249a43c59f8ccf1040713ec8","c79844fee0ed40249611dbd3e3276509","c1204727b6b944f39cfbb388d681c252","2b6c89c1b37a429282a76277288b379c","76b14fd52ec94af593c9af71413ce1dd","ce50ec05d23d42b885cc709361a6f6ad"]},"id":"JALQ-nzDaHBc","executionInfo":{"status":"ok","timestamp":1637218263732,"user_tz":-330,"elapsed":1774706,"user":{"displayName":"Aryamaan Jain","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKvxqXMjfEQPe4K-2M9GvOVADF5IAW14NQ8-c2Yw=s64","userId":"18320413729977503417"}},"outputId":"e6ab2f3e-d67f-4a56-c1b3-22b6e2ceddd5"},"source":["# emotions = [\"anger\", \"fear\", \"joy\", \"sadness\"]\n","X_train_a, y_train_a, _, _ = load_dataset(\"anger\")\n","X_train_f, y_train_f, _, _ = load_dataset(\"fear\")\n","X_train_j, y_train_j, _, _ = load_dataset(\"joy\")\n","\n","X_train = X_train_a + X_train_f + X_train_j\n","y_train = np.concatenate((y_train_a, y_train_f, y_train_j))\n","X_test, y_test, _, _ = load_dataset(\"sadness\")\n","\n","training_set = Triage(X_train, y_train, tokenizer, MAX_LEN)\n","testing_set = Triage(X_test, y_test, tokenizer, MAX_LEN)\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)\n","\n","model = RobertaBERTClass()\n","model.to(device)\n","optimizer = torch.optim.Adam(params= model.parameters(), lr=LEARNING_RATE)\n","\n","for epoch in range(EPOCHS):\n","    print('Epoch:', epoch)\n","    train(epoch)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21136d4c2ad941b3b1dcbaf402a0974b","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0\n","Training Loss Epoch: 0.04891765859558562\n","pearsonr score: 0.33466460429743394\n","Epoch: 1\n","Training Loss Epoch: 0.033915344304603315\n","pearsonr score: 0.46922418872267235\n","Epoch: 2\n","Training Loss Epoch: 0.025245279555103858\n","pearsonr score: 0.5505417326710543\n","Epoch: 3\n","Training Loss Epoch: 0.019796560491301535\n","pearsonr score: 0.4986912975705796\n"]}]}]}